{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcZ9ocCW5Cnb",
        "outputId": "7c2dda19-95c8-4efc-b13a-17a5cf43ec68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (4.9.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (2022.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23->pandas-datareader) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from multipledispatch) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade pandas-datareader\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "from prettytable import PrettyTable\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas_datareader.data as web\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install multipledispatch\n",
        "from multipledispatch import dispatch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_from_web(ticker):\n",
        "    # NSE was incorporated in 1992. It was recognised as a stock exchange by SEBI in April 1993 and commenced operations in 1994\n",
        "    print(\"Getting the data for the ticker \", ticker)\n",
        "    start = date(1994, 1, 1)\n",
        "    yesterday = date.today() - timedelta(days=1)\n",
        "    df = web.DataReader(ticker, 'yahoo', start=start, end=yesterday)\n",
        "    return df\n",
        "\n",
        "\n",
        "def linear(x_train, y_train, x_test, y_test):\n",
        "\n",
        "    # hyper-paramater tuning\n",
        "    clf = SGDRegressor(loss=\"squared_error\", penalty=\"l2\")\n",
        "    values = [10**i for i in range(-10, 6)]\n",
        "    hyper_parameter = {\"alpha\": values}\n",
        "    gscv = GridSearchCV(\n",
        "        clf, hyper_parameter, scoring=\"neg_mean_squared_error\", cv=10, verbose=1, n_jobs=-1)\n",
        "    gscv.fit(x_train, y_train)\n",
        "    alpha = gscv.best_params_[\"alpha\"]\n",
        "\n",
        "    # applying linear regression with optimal hyper-parameter\n",
        "    clf = SGDRegressor(loss=\"squared_error\", penalty=\"l2\", alpha=alpha)\n",
        "    clf.fit(x_train, y_train)\n",
        "\n",
        "    train_pred = clf.predict(x_train)\n",
        "    train_MAPE = mean_absolute_error(\n",
        "        y_train, train_pred) / (sum(y_train)/len(y_train))\n",
        "    train_MSE = mean_squared_error(y_train, train_pred)\n",
        "\n",
        "    test_pred = clf.predict(x_test)\n",
        "    test_MAPE = mean_absolute_error(\n",
        "        y_test, test_pred) / (sum(y_test)/len(y_test))\n",
        "    test_MSE = mean_squared_error(y_test, test_pred)\n",
        "\n",
        "    return train_pred, test_pred, train_MAPE, train_MSE, test_MAPE, test_MSE\n",
        "\n",
        "\n",
        "def randomForest(x_train, y_train, x_test, y_test):\n",
        "\n",
        "    # hyper-paramater tuning\n",
        "    clf = RandomForestRegressor(n_jobs=-1)\n",
        "    hyper_parameter = {\"n_estimators\": [10, 50, 100, 500],\n",
        "                       \"max_depth\": [1, 5, 10, 50, 100, 500, 1000]}\n",
        "    gscv = GridSearchCV(\n",
        "        clf, hyper_parameter, scoring=\"neg_mean_squared_error\", cv=3, verbose=1, n_jobs=-1)\n",
        "    gscv.fit(x_train, y_train)\n",
        "    estimators = gscv.best_params_[\"n_estimators\"]\n",
        "    max_depth = gscv.best_params_[\"max_depth\"]\n",
        "\n",
        "    # applying random forest with optimal hyper-parameter\n",
        "    clf = RandomForestRegressor(\n",
        "        n_estimators=estimators, max_depth=max_depth, verbose=1, n_jobs=-1)\n",
        "    clf.fit(x_train, y_train)\n",
        "\n",
        "    train_pred = clf.predict(x_train)\n",
        "    train_MAPE = mean_absolute_error(\n",
        "        y_train, train_pred) / (sum(y_train)/len(y_train))\n",
        "    train_MSE = mean_squared_error(y_train, train_pred)\n",
        "\n",
        "    test_pred = clf.predict(x_test)\n",
        "    test_MAPE = mean_absolute_error(\n",
        "        y_test, test_pred) / (sum(y_test)/len(y_test))\n",
        "    test_MSE = mean_squared_error(y_test, test_pred)\n",
        "\n",
        "    return train_pred, test_pred, train_MAPE, train_MSE, test_MAPE, test_MSE\n",
        "\n",
        "\n",
        "def xgboost_reg(x_train, y_train, x_test, y_test):\n",
        "    # hyper-parameter tuning\n",
        "    hyper_parameter = {\"max_depth\": [1, 2, 3, 4],\n",
        "                       \"n_estimators\": [10, 50, 100, 500]}\n",
        "    clf = xgb.XGBRegressor(silent=True)\n",
        "    best_parameter = GridSearchCV(\n",
        "        clf, hyper_parameter, scoring=\"neg_mean_squared_error\", cv=3)\n",
        "    best_parameter.fit(x_train, y_train)\n",
        "    estimators = best_parameter.best_params_[\"n_estimators\"]\n",
        "    depth = best_parameter.best_params_[\"max_depth\"]\n",
        "\n",
        "    # applying xgboost regressor with best hyper-parameter\n",
        "    clf = xgb.XGBRegressor(\n",
        "        max_depth=depth, n_estimators=estimators, silent=True)\n",
        "    clf.fit(x_train, y_train)\n",
        "\n",
        "    train_pred = clf.predict(x_train)\n",
        "    train_MAPE = mean_absolute_error(\n",
        "        y_train, train_pred) / (sum(y_train)/len(y_train))\n",
        "    train_MSE = mean_squared_error(y_train, train_pred)\n",
        "\n",
        "    test_pred = clf.predict(x_test)\n",
        "    test_MAPE = mean_absolute_error(\n",
        "        y_test, test_pred) / (sum(y_test)/len(y_test))\n",
        "    test_MSE = mean_squared_error(y_test, test_pred)\n",
        "\n",
        "    return train_pred, test_pred, train_MAPE, train_MSE, test_MAPE, test_MSE"
      ],
      "metadata": {
        "id": "qlLq5ncU5Hco"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dispatch(object, object, object)\n",
        "def plot(data1, d1_label, title=\"\"):\n",
        "    fig = plt.figure(figsize=(25, 8))\n",
        "    plt.title(title)\n",
        "    plt.plot(data1, 'b', label=d1_label)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "@dispatch(object, object, object, object, object)\n",
        "def plot(data1, d1_label, data2, d2_label, title=\"\"):\n",
        "    fig = plt.figure(figsize=(25, 8))\n",
        "    plt.title(title)\n",
        "    plt.plot(data1, 'b', label=d1_label)\n",
        "    plt.plot(data2, 'b', label=d2_label)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sSP_eGEt5uNT"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(data, split_sz):\n",
        "    data_train = pd.DataFrame(data['Close'][0: int(len(data)*split_sz)])\n",
        "    data_test = pd.DataFrame(data['Close'][int(len(data)*split_sz): int(len(data))])\n",
        "\n",
        "    print(\"Size of complete data = \", data.shape)\n",
        "    print(\"Size of training data = \", data_train.shape)\n",
        "    print(\"Size of testing data = \", data_test.shape)\n",
        "\n",
        "    return data_train, data_test\n",
        "    \n",
        "def prepare_train_set(train_data, win_sz):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    train_data_arr = scaler.fit_transform(train_data)\n",
        "    \n",
        "    train_data_dt = data_train.reset_index()[\"Date\"]\n",
        "    train_x = []\n",
        "    train_y = []\n",
        "    train_t = []\n",
        "\n",
        "    for i in range(win_sz, len(train_data_arr)):\n",
        "        train_x.append(train_data_arr[i-win_sz : i, 0])\n",
        "        train_y.append(train_data_arr[i, 0])\n",
        "        train_t.append(train_data_dt[i])\n",
        "\n",
        "    train_x, train_y, train_t = np.array(train_x), np.array(train_y), np.array(train_t)\n",
        "    print(\"Shape of train feature set \", train_x.shape)\n",
        "    print(\"Shape of train target \", train_y.shape)\n",
        "    print(\"Shape of train timestamp \", train_t.shape)\n",
        "    \n",
        "    return train_x, train_y, train_t\n",
        "\n",
        "def prepare_test_set(past_n_days_data, test_data, win_sz):\n",
        "    test_data = pd.concat([past_n_days_data, test_data])\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    test_data_arr = scaler.fit_transform(test_data)\n",
        "    \n",
        "    test_data_dt = test_data.reset_index()[\"Date\"]\n",
        "    test_x = []\n",
        "    test_y = []\n",
        "    test_t = []\n",
        "\n",
        "    for i in range(win_sz, len(test_data_arr)):\n",
        "        test_x.append(test_data_arr[i-win_sz: i, 0])\n",
        "        test_y.append(test_data_arr[i, 0])\n",
        "        test_t.append(test_data_dt[i])\n",
        "\n",
        "    test_x, test_y, test_t = np.array(test_x), np.array(test_y), np.array(test_t)\n",
        "    print(\"Size of train dataset : \", test_x.shape)\n",
        "    print(\"Size of test dataset : \", test_y.shape)\n",
        "    print(\"Size of test timestamp : \", test_t.shape)\n",
        "\n",
        "    return test_x, test_y, test_t"
      ],
      "metadata": {
        "id": "YFKL08Y5_ON_"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xZY_n7oC5t_N"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Shs8OR3R5dhO"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dFfeU2E45s5m"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ticker = \"RELIANCE.NS\"\n",
        "data = get_data_from_web(ticker)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQL8M6eQ5t3O",
        "outputId": "8cf4e9a7-7878-402b-d32e-bf53c4f37269"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting the data for the ticker  RELIANCE.NS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train, data_test = train_test_split(data, split_sz=0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AheLepv5uVn",
        "outputId": "183d4f6e-d7f6-4330-db72-1bc6bf795e1a"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of complete data =  (6716, 6)\n",
            "Size of training data =  (4701, 1)\n",
            "Size of testing data =  (2015, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "win_size = 50\n",
        "train_x, train_y, train_t = prepare_train_set(data_train, win_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z7L3d-XCC9U",
        "outputId": "53323496-b704-4888-dac7-1bc9abf9d3b2"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train feature set  (4651, 50)\n",
            "Shape of train target  (4651,)\n",
            "Shape of train timestamp  (4651,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "past_n_days_data = data_train.tail(win_size)\n",
        "test_x, test_y, test_t = prepare_test_set(past_n_days_data, data_test, win_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kEpZoX3PKds",
        "outputId": "285b1cf7-b959-4887-c398-df6a3cd09988"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of train dataset :  (2015, 50)\n",
            "Size of test dataset :  (2015,)\n",
            "Size of test timestamp :  (2015,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicting using linear regression model...\")\n",
        "train_y_hat_lr, test_y_hat_lr, trainMAPE_lr, trainMSE_lr, testMAPE_lr, testMSE_lr = linear(train_x, train_y, test_x, test_y)\n",
        "print(\"Predicting using randomforest regression model...\")\n",
        "tain_y_hat_rf, test_y_hat_rf, trainMAPE_rf, trainMSE_rf, testMAPE_rf, testMSE_rf = randomForest(train_x, train_y, test_x, test_y)\n",
        "print(\"Predicting using xgboost regression model...\")\n",
        "tain_y_hat_xgb, test_y_hat_xgb, trainMAPE_xgb, trainMSE_xgb, testMAPE_xgb, testMSE_xgb = xgboost_reg(train_x, train_y, test_x, test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCU7O6vwPeu4",
        "outputId": "6f1f844b-be4f-42ae-ab3f-0e1d3a048106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting using linear regression model...\n",
            "Fitting 10 folds for each of 16 candidates, totalling 160 fits\n",
            "Predicting using randomforest regression model...\n",
            "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z64NBagpQjTW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}